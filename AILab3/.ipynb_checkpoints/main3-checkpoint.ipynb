{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzp1CvZ7SxrW"
   },
   "source": [
    "# Лабораторная работа №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_YL60YYSxrY"
   },
   "source": [
    "## Вариант №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fiXo4_fUSxrZ"
   },
   "source": [
    "Проза на английском языке, элемент последовательности - одно слово. Источник данных - тексты c https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLaTnSAXSxrd"
   },
   "source": [
    "## Чистка текста и разбиение на последовательности слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50271,
     "status": "ok",
     "timestamp": 1598391339182,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "SAiin0JviPap",
    "outputId": "dce58181-dc06-442d-97c3-fd37f9fca078"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/stdneprov/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wPhQZ_-Sxre"
   },
   "source": [
    "Получим спискок из текстов всех книг в папке books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50226,
     "status": "ok",
     "timestamp": 1598391339185,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "QHpuvY7XSxrf"
   },
   "outputs": [],
   "source": [
    "def BooksOpen(dirrectory='./books/'):\n",
    "    bookNames = os.listdir(dirrectory)\n",
    "    if '.DS_Store' in bookNames:\n",
    "        bookNames.remove('.DS_Store')\n",
    "    bookTexts = [open(dirrectory + name).read() for name in bookNames]\n",
    "    return bookTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_GIr83xSxri"
   },
   "source": [
    "Уберетм допольнительую информацию в начале книги (включая оглавление) и в конце, после завершения повествования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50188,
     "status": "ok",
     "timestamp": 1598391339187,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "zGlFgoFOSxri"
   },
   "outputs": [],
   "source": [
    "def RemoveAditionalText(bookTexts):\n",
    "    for i in range(len(bookTexts)):\n",
    "        for j in range(2):\n",
    "            j = (re.search('\\n*chapter\\s[i, 1]\\.*\\s+.*\\n', bookTexts[i], flags=re.IGNORECASE))\n",
    "            if j is None:\n",
    "                break\n",
    "            j = j.span()[1] + 1\n",
    "            bookTexts[i] = bookTexts[i][j:]\n",
    "        bookTexts[i] = bookTexts[i][:bookTexts[i].find('*** END OF')]\n",
    "    return bookTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-kShVxbSxrl"
   },
   "source": [
    "Разобьем книгу на главы и получим список текстов глав всех книг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50181,
     "status": "ok",
     "timestamp": 1598391339191,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "IUBelGagSxrl"
   },
   "outputs": [],
   "source": [
    "def SplitToChapters(bookTexts):\n",
    "    chapterTexts = []\n",
    "    for bookText in bookTexts:\n",
    "        while True:\n",
    "            j = re.search('\\n*chapter+.*\\n', bookText, flags=re.IGNORECASE)\n",
    "            if j is None:\n",
    "                if len(bookText) > 500:\n",
    "                    chapterTexts.append(bookText)\n",
    "                break\n",
    "            if j.span()[0] > 500:\n",
    "                chapterTexts.append(bookText[:j.span()[0]])\n",
    "            bookText = bookText[j.span()[1] + 1:]\n",
    "    return chapterTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqASfYqKSxro"
   },
   "source": [
    "Теперь токенизируем каждую главу, приводя все слова к нижнему регистру, кроме имен, проверяя принадленость слов к тому или иному типу при помощи nltk.pos_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50172,
     "status": "ok",
     "timestamp": 1598391339193,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "5OsK83bnSxrp"
   },
   "outputs": [],
   "source": [
    "def SplitToWords(chapterTexts):\n",
    "    words = []\n",
    "    for chapterText in chapterTexts:\n",
    "        markedWords = nltk.pos_tag(nltk.word_tokenize(chapterText))\n",
    "        chapterWords = [(markedWord[0] if markedWord[1] == 'NNP' else markedWord[0].lower()) for markedWord in markedWords]\n",
    "        words.append(chapterWords)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NzS3E_CSxrr"
   },
   "source": [
    "Каждую галву разбиваем на цепочки по 50 слов, после чего объединяем эти цепочки в список. Такая монипуляция помогает обрезать конец главы, чтобы он не влиял на начало другой главы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50164,
     "status": "ok",
     "timestamp": 1598391339195,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "8pusjmOvSxrs"
   },
   "outputs": [],
   "source": [
    "def WordsToСhains(words, chainLen=51):\n",
    "    wordChains = []\n",
    "    for chapterWords in words:\n",
    "        for i in range(chainLen, len(chapterWords), chainLen):\n",
    "            wordChains.append(chapterWords[i - chainLen : i])\n",
    "    return wordChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZJa0laZSxrv"
   },
   "source": [
    "Теперь запустим все ранее описнные функции и получим списко слов, с которым будем работать в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107028,
     "status": "ok",
     "timestamp": 1598391396069,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "WsBv6gHjSxrw",
    "outputId": "40693a5f-8571-4125-d1fc-2e73b6f7572f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Из 5 книг я получил 384 глав, из которых извлек 23037 цепочек слов.\n"
     ]
    }
   ],
   "source": [
    "bookTexts = RemoveAditionalText(BooksOpen())\n",
    "chapterTexts = SplitToChapters(bookTexts)\n",
    "words = SplitToWords(chapterTexts)\n",
    "wordChains = WordsToСhains(words)\n",
    "\n",
    "print(f'Из {len(bookTexts)} книг я получил {len(chapterTexts)} глав, из которых извлек {len(wordChains)} цепочек слов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwXZXWU5Sxrz"
   },
   "source": [
    "## Создание обучающего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109122,
     "status": "ok",
     "timestamp": 1598391398179,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "lnwDrinBlHrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PGjXmUCSxr0"
   },
   "source": [
    "Получим структуры getWord и getID для биективного отбражения слова в индекс и наоборот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109111,
     "status": "ok",
     "timestamp": 1598391398180,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "KyFzoBGnSxr0"
   },
   "outputs": [],
   "source": [
    "def GetWordsWithID(wordMatrix):\n",
    "    getWord = np.array(list(wordMatrix.wv.vocab))\n",
    "    getID = {}\n",
    "    for i, word in zip(range(getWord.size), getWord):\n",
    "        getID[word] = i\n",
    "    return getWord, getID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8kzc5lISxr3"
   },
   "source": [
    "Найдем самое часто встречающееся слово, это нужно для замены слов, которых нет в словаре (работает очень долго)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109094,
     "status": "ok",
     "timestamp": 1598391398181,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "SCFYel-PSxr3"
   },
   "outputs": [],
   "source": [
    "def GetMostCommonWord(wordChains):\n",
    "    words = []\n",
    "    for word in wordChains:\n",
    "        words.extend(word)\n",
    "    return max(words, key=words.count)\n",
    "#GetMostCommonWord(wordChains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHgi2uQPSxr7"
   },
   "source": [
    "Переведем последовательность слов в последовательность индексов для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108954,
     "status": "ok",
     "timestamp": 1598391398184,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "aC2tklt8Sxr7"
   },
   "outputs": [],
   "source": [
    "def WordListToIndexes(wordChains, getID, mostCommonWord='as'):\n",
    "    indexes = [[getID[word] if word in getID else getID[mostCommonWord] for word in words] for words in wordChains]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WfK9qmwSxr-"
   },
   "source": [
    "Получаем предобученную матрицу слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 175164,
     "status": "ok",
     "timestamp": 1598391464424,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "ZKc3sp6gSxr-"
   },
   "outputs": [],
   "source": [
    "wordMatrix = Word2Vec(wordChains, min_count=3, size = 500, workers = 4, iter = 27, alpha = 0.1)\n",
    "\n",
    "getWord, getID = GetWordsWithID(wordMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('wordMatrix', 'wb') as f:\n",
    "    pickle.dump(wordMatrix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUtAGqZSxsE"
   },
   "source": [
    "Получим последовательности чисел и преобразуем их в тензорный вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179496,
     "status": "ok",
     "timestamp": 1598391468767,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "jCK0AWYESxsF"
   },
   "outputs": [],
   "source": [
    "df = tf.data.Dataset.from_tensor_slices(WordListToIndexes(wordChains, getID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzRqV13tSxsH"
   },
   "source": [
    "Код из учебника по tensorlow: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "Для каждой последовательности продублируем и сдвинем ее, чтобы сформировать входной и целевой текст, используя метод map для применения простой функции к каждому пакету, после чего перемешиваем данные и упаковываем их в пакеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179474,
     "status": "ok",
     "timestamp": 1598391468769,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "ISWVIb4bSxsI",
    "outputId": "9d44246d-61fc-4eca-d42d-2a17e5d47912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((100, 50), (100, 50)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "df = df.map(split_input_target)\n",
    "df = df.shuffle(10000).batch(100, drop_remainder=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se3HQnYCSxsL"
   },
   "source": [
    "## Описание моделей и их обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179465,
     "status": "ok",
     "timestamp": 1598391468769,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "XsqjjtiPmJIz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, LSTM, BatchNormalization, Dropout, Dense, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSQ5Pk18lvrP"
   },
   "source": [
    "### Функции потерь и создания чекпоинтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179403,
     "status": "ok",
     "timestamp": 1598391468770,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "d9n8dyMNSxsL"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179390,
     "status": "ok",
     "timestamp": 1598391468771,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "SOsvTTJKSxsO"
   },
   "outputs": [],
   "source": [
    "def MakeCheckpoint(checkpoint_dir = \"./\"):\n",
    "    filepath = os.path.join(checkpoint_dir, \"checkpoints/ckpt_model\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"loss\", filepath=filepath, save_weights_only=True, save_best_only=True)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnQlIdy8SxsU"
   },
   "source": [
    "### Полносвязная рекурентная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179371,
     "status": "ok",
     "timestamp": 1598391468772,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "ikcaxyEvSxsV"
   },
   "outputs": [],
   "source": [
    "def RNN(wordMatrix, batchSize=100, units=512):\n",
    "    return Sequential([\n",
    "        Embedding(len(wordMatrix.wv.vocab), wordMatrix.vector_size, batch_input_shape=[batchSize, None], weights=[wordMatrix.wv.vectors]),\n",
    "        SimpleRNN(units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(len(wordMatrix.wv.vocab), kernel_initializer=\"glorot_uniform\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179850,
     "status": "ok",
     "timestamp": 1598391469276,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "ILVZKbn6SxsX",
    "outputId": "55d94b3a-9aa8-4e20-b2c7-0d0116ce2e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (100, None, 500)          7444000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (100, None, 512)          518656    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, None, 14888)        7637544   \n",
      "=================================================================\n",
      "Total params: 15,600,200\n",
      "Trainable params: 15,600,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(wordMatrix)\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179831,
     "status": "ok",
     "timestamp": 1598391469279,
     "user": {
      "displayName": "Vanya Dneprov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9lCPUMCUAXZG0rfScjILqa6X_b0F9R-0ge-kTig=s64",
      "userId": "04935545358614125790"
     },
     "user_tz": -180
    },
    "id": "eJc2vydaSxsZ"
   },
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Qcf3eIWtSxsb",
    "outputId": "d04a4aa5-9830-4d8f-a3ec-202626f816cf"
   },
   "source": [
    "callbacks = [EarlyStopping(monitor=\"loss\", patience=3), MakeCheckpoint('./RNN/')]\n",
    "rnnEpochLoss = rnn.fit(df, epochs=15, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(wordMatrix, 1)\n",
    "rnn.load_weights(tf.train.latest_checkpoint(os.path.join(\"./RNN/checkpoints/\")))\n",
    "rnn.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save(os.path.join(\"./RNN/rnn.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_int(word, word2idx, default = 'as'):\n",
    "    if word in word2idx:\n",
    "        return word2idx[word]\n",
    "    return word2idx[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_text(words):\n",
    "  ans = \"\"\n",
    "  higher = False\n",
    "  puncts = {\".\", \",\", \":\", \"!\", \"?\", \";\", '\"', \"'\", \"’\"}\n",
    "  enders = {\".\", \"?\", \"!\"}\n",
    "  for word in words:\n",
    "    if higher:\n",
    "      word = word.capitalize()\n",
    "    if word in puncts:\n",
    "      ans += word\n",
    "    else:\n",
    "      ans += \" \" + word\n",
    "    \n",
    "    higher = False\n",
    "    if word in enders:\n",
    "      higher = True\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, word2idx, idx2word, start_string, num_generate=20):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  seq = SplitToWords(start_string)\n",
    "\n",
    "  converter = lambda x: word_to_int(x, word2idx)\n",
    "  input_eval = list(map(converter, seq))\n",
    "\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.25\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "  return start_string + list_to_text(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b170d0fae9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetWord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I predict that it will be\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-9496c7d44612>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, word2idx, idx2word, start_string, num_generate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0minput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0minput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-9496c7d44612>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitToWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0minput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-b3e44a4733aa>\u001b[0m in \u001b[0;36mword_to_int\u001b[0;34m(word, word2idx, default)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'as'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "generate_text(rnn, getID, getWord, \"I predict that it will be\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIIA84EjSxse"
   },
   "source": [
    "### Однослойная нейросеть долгой краткосрочной памяти\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1lA_RUkgUdA"
   },
   "outputs": [],
   "source": [
    "def SingleLSTM(wordMatrix, batchSize=100, units = 512):\n",
    "    return Sequential([\n",
    "        Embedding(len(wordMatrix.wv.vocab), wordMatrix.vector_size, batch_input_shape=[batchSize, None], weights=[wordMatrix.wv.vectors]),\n",
    "        LSTM(units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(len(wordMatrix.wv.vocab), kernel_initializer=\"glorot_uniform\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaMHwHNzgUdD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (100, None, 500)          7444000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (100, None, 512)          2074624   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, None, 14888)        7637544   \n",
      "=================================================================\n",
      "Total params: 17,156,168\n",
      "Trainable params: 17,156,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "singleLSTM = SingleLSTM(wordMatrix)\n",
    "print(singleLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GncLbCPkgUdH"
   },
   "outputs": [],
   "source": [
    "singleLSTM.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gxBKbk3gUdM"
   },
   "source": [
    "callbacks = [EarlyStopping(monitor=\"loss\", patience=3), MakeCheckpoint('./SingleLSTM/')]\n",
    "singleLSTMEpochLoss = singleLSTM.fit(df, epochs=15, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PH9UsYpbkF-P"
   },
   "source": [
    "### Двухслойная нейросеть долгой краткосрочной памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ7e1IyXjkaE"
   },
   "outputs": [],
   "source": [
    "def DoubleLSTM(wordMatrix, batchSize=100, units = 512):\n",
    "    return Sequential([\n",
    "        Embedding(len(wordMatrix.wv.vocab), wordMatrix.vector_size, batch_input_shape=[batchSize, None], weights=[wordMatrix.wv.vectors]),\n",
    "        LSTM(units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
    "        LSTM(units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(len(wordMatrix.wv.vocab), kernel_initializer=\"glorot_uniform\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97uV8OzqjkaM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (100, None, 500)          7444000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (100, None, 512)          2074624   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (100, None, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, None, 14888)        7637544   \n",
      "=================================================================\n",
      "Total params: 19,255,368\n",
      "Trainable params: 19,255,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "doubleLSTM = DoubleLSTM(wordMatrix)\n",
    "print(doubleLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0sZfeYBjkaP"
   },
   "outputs": [],
   "source": [
    "doubleLSTM.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyd4rF2pjkaS"
   },
   "source": [
    "callbacks = [EarlyStopping(monitor=\"loss\", patience=3), MakeCheckpoint('./DoubleLSTM/')]\n",
    "doubleLSTMEpochLoss = doubleLSTM.fit(df, epochs=15, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ToOapI1k1tU"
   },
   "source": [
    "### Однослойный управляемый рекурентный нейрон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urHOH00wjxvR"
   },
   "outputs": [],
   "source": [
    "def SingleGRU(wordMatrix, batchSize=100, units=512):\n",
    "    return Sequential([\n",
    "        Embedding(len(wordMatrix.wv.vocab), wordMatrix.vector_size, batch_input_shape=[batchSize, None], weights=[wordMatrix.wv.vectors]),\n",
    "        GRU(units, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(len(wordMatrix.wv.vocab), kernel_initializer=\"glorot_uniform\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZTet9m8jxvW"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GRU() got an unexpected keyword argument 'return_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c95459de19c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-4ad4b8b4a56e>\u001b[0m in \u001b[0;36mGRU\u001b[0;34m(wordMatrix, batchSize, units)\u001b[0m\n\u001b[1;32m      2\u001b[0m     return Sequential([\n\u001b[1;32m      3\u001b[0m         \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         Dense(len(wordMatrix.wv.vocab), kernel_initializer=\"glorot_uniform\")])\n",
      "\u001b[0;31mTypeError\u001b[0m: GRU() got an unexpected keyword argument 'return_sequences'"
     ]
    }
   ],
   "source": [
    "gru = SingleGRU(wordMatrix)\n",
    "print(gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQ-yeZyIjxvY"
   },
   "outputs": [],
   "source": [
    "gru.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_Srn0Xzjxvc"
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor=\"loss\", patience=3), MakeCheckpoint('./GRU/')]\n",
    "gruEpochLoss = gru.fit(df, epochs=15, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
